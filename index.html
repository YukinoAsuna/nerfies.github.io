<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Jailbreak, LLM, Foot-In-The-Door">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Foot-In-The-Door: A Multi-turnJailbreak for LLMs</title>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  <script async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    /* 默认收拢状态 */
    .expandable {
      max-height: 0;  /* 确保内容初始不可见 */
      overflow: hidden;
      transition: max-height 0.4s ease-in-out, padding 0.4s ease-in-out;
      padding: 0 15px; /* 初始状态去除内边距，防止占位 */
    }

    /* 鼠标悬停时展开 */
    .experiment-card:hover .expandable {
      max-height: 800px; /* 设定足够大的高度，确保完整展开 */
      padding: 15px; /* 让内边距恢复，避免视觉突兀 */
    }

    /* 卡片样式 */
    .experiment-card {
      background: white;
      border-radius: 10px;
      box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1);
      transition: all 0.3s ease-in-out;
      padding: 15px;
      cursor: pointer;
      margin: 20px auto;
      max-width: 800px;
    }

    /* 头部标题样式 */
    .experiment-header {
      background-color: #87CEFA;
      color: white;
      padding: 15px;
      font-weight: bold;
      text-align: center;
      border-radius: 5px;
      margin-bottom: 10px;
    }

    /* 表格区域 */
    .experiment-table {
      width: 100%;
      overflow-x: auto;
    }

    /* 表格样式 */
    .experiment-table table {
      border-collapse: collapse;
      width: 100%;
      text-align: center;
    }

    /* 表格单元格 */
    .experiment-table th, .experiment-table td {
      border: 1px solid black;
      padding: 10px;
    }

    /* 表头背景色 */
    .experiment-table th {
      background-color: #f2f2f2;
    }
  </style>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/FITD.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">Foot-In-The-Door: A Multi-turnJailbreak for LLMs</h1> -->
          <h1 class="title is-1 publication-title">
            <img src="./static/images/FITD.jpg" alt="Logo" class="title-logo">
            Foot-In-The-Door: A Multi-turn Jailbreak for LLMs
          </h1>
        
        
          <div class="is-size-5 publication-authors" style="display: flex; justify-content: center; gap: 50px; font-weight: bold;">
            <span class="author-block">Zixuan Weng</span>
            <span class="author-block">Xiaolong Jin</span>
            <span class="author-block">Jinyuan Jia</span>
            <span class="author-block">Xiangyu Zhang</span>
          </div>
        
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Google Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- 嵌入 jpg 图片 -->
      <!-- <img src="./static/images/intro_v2.jpg" alt="Introduction image" style="max-width: 80%; border: 1px solid #ddd; box-shadow: 2px 2px 10px rgba(0,0,0,0.1);"/> -->
      <img src="./static/images/intro.jpg"
                 class="interpolation-image"
                 alt="Introduction image."/>
      <h2 class="subtitle has-text-centered">
        <!-- <span class="dnerf">An example of FITD about hacking into an email account compared to a direct query. It bypasses alignment as the malicious intent escalates over multiple interactions.</span> -->
        An example of <strong><span class="dnerf">FITD</span></strong> about hacking into an email account compared to a direct query. It bypasses alignment as the malicious intent escalates over multiple interactions.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. 
            A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs.
        </p>
        <p>
            Inspired by psychological foot-in-the-door principles, we introduce <strong><span class="dnerf">FITD</span></strong>, 
            a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments 
            lower resistance to more significant or more unethical transgressions.
            Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts 
            and aligns the model's response by itself to induce toxic responses.
        </p>
        <p>
            Extensive experimental results on two jailbreak benchmarks demonstrate that <strong><span class="dnerf">FITD</span></strong> 
            achieves an average attack success rate of 94% across seven widely used models, 
            outperforming existing state-of-the-art methods.
            Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in 
            current alignment strategies and emphasizing the risks inherent in multi-turn interactions.
        </p>
        
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">FITD: Foot-In-The-Door Multi-turn Jailbreak Method</h2>
        <div class="content has-text-justified">
          
          <!-- <img src="./static/images/main.jpg"
          alt="Introduction image."/> -->
          <!-- <div style="text-align: center;">
            <img src="./static/images/main.jpg" 
            alt="FITD image." 
            style="width: 125%; max-width: 2000px; height: auto;"/>
          </div> -->
          <div style="width: 100%; overflow: hidden; display: flex; justify-content: center; align-items: center;">
            <img src="./static/images/main.jpg" 
                 alt="FITD image."
                 style="width: 100%; max-width: 3000px; height: auto; object-fit: contain;">
          </div>          
          
          
          
          <!-- <p class="subtitle has-text-centered" style="font-size: 1.2rem; line-height: 1.5;">
            Overview of <span class="dnerf" style="font-size: 1.1rem;">FITD</span>.
            The attack begins by generating Level \( 1 \) to Level \( n \) queries by an assistant model. 
            Through multi-turn interactions, self-corruption is enhanced via 
            <span class="module" style="font-size: 1.1rem; font-weight: bold;">SSParaphrase</span> 
            and <span class="module" style="font-size: 1.1rem; font-weight: bold;">Re-Align</span>, 
            ensuring the attack remains effective.
          
            <span class="module" style="font-size: 1.1rem;">SSParaphrase (SlipperySlopeParaphrase)</span> 
            refines queries by generating intermediate malicious-level queries 
            \( q_{\text{mid}} \) between \( q_{\text{last}} \) and \( q_i \).
          
            <span class="module" style="font-size: 1.1rem;">Re-Align</span> uses prompt 
            \( p_{\text{align}} \) to align the target model's responses \( r_{\text{align}} \).
          </p> -->
          <p class="subtitle has-text-centered" style="font-size: 1rem; line-height: 1.4; margin-bottom: 40px;">
            Overview of <strong><span class="dnerf" style="font-size: 0.9rem;">FITD</span></strong>.
            The attack begins by generating Level \( 1 \) to Level \( n \) queries by an assistant model. 
            Through multi-turn interactions, self-corruption is enhanced via 
            <span class="module" style="font-size: 0.9rem; font-weight: bold;">Re-Align</span>
            and <span class="module" style="font-size: 0.9rem; font-weight: bold;">SSParaphrase</span>, 
            ensuring the attack remains effective.
            
            <span class="module" style="font-size: 0.9rem;">Re-Align</span> uses prompt 
            \( p_{\text{align}} \) to align the target model's responses \( r_{\text{align}} \).

            <span class="module" style="font-size: 0.9rem;">SSParaphrase (SlipperySlopeParaphrase)</span> 
            refines queries by generating intermediate malicious-level queries 
            \( q_{\text{mid}} \) between \( q_{\text{last}} \) and \( q_i \).
            
          </p>
          
          <h3 class="title is-4">Inspiration from Psychology: The Foot-in-the-Door Phenomenon</h3>
          <div class="content has-text-justified">
            <p>
              Our method <strong><span class="dnerf">FITD</span></strong> draws inspiration from the "foot-in-the-door" phenomenon in psychology. 
              According to this principle, once individuals perform or agree to a minor (often unethical) act, they are more likely to proceed with more 
              significant or harmful acts afterward. 
              For example, in a classic study, participants who first displayed a small sign supporting safe driving were subsequently much 
              more inclined to install a much larger, more obtrusive sign. This gradual escalation of compliance, 
              "from small to large", has also been observed in other forms of unethical or harmful behavior, showing that the initial "small step" often 
              lowers psychological barriers for larger transgressions. Once a small unethical act has been justified, individuals become increasingly 
              susceptible to more severe transgressions.
            </p>
            <p>
              Based on these insights, we hypothesize that LLMs' safety mechanisms might be vulnerable to a gradual escalation strategy. 
              If LLMs respond to a prompt containing slightly harmful content, subsequent queries that escalate this harmfulness will have 
              a higher chance of producing disallowed responses. This idea underlies our <strong><span class="dnerf">FITD</span></strong> method, 
              which progressively coaxes a target model to produce increasingly malicious output despite its built-in safety mechanisms.
            </p>
          
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <div class="columns" style="display: flex; align-items: stretch; gap: 20px;">
      <!-- Re-Align Section -->
      <div class="column" style="display: flex; flex-direction: column;">
        <div class="content">
          <h2 class="title is-3">Re-Align</h2>
          <p>
            If the model's previous query \( q_{\text{last}} \) and response \( r_{\text{last}} \) in chat history \( \mathcal{H} \) is misaligned—for instance, it remains too benign or partially refuses even though the query is not malicious—then we invoke <b>Re-Align</b>. Building on the psychological insight that once individuals have justified a minor unethical act, they become increasingly susceptible to more severe transgressions, <b>Re-Align</b> aims to "nudge" the model to produce a response more closely aligned with the malicious intent of \( q_{\text{last}} \).
          </p>
          <p>
            Specifically, we employ a predefined alignment prompt \( p_{\text{align}} \) via \( \texttt{getAlignPrompt}(q_{\text{last}}, r_{\text{last}}) \), appending it to \( \mathcal{H} \) before querying the model \( \mathcal{T} \) again. 
            The alignment prompt explicitly points out inconsistencies between the last query \( q_{\text{last}} \) and response \( r_{\text{last}} \) while encouraging the model to stay consistent with multi-turn conversation. For example, if \( r_{\text{last}} \) is too cautious or is in partial refusal, \( p_{\text{align}} \) will suggest that the model refines its response to better follow the implicit direction.
          </p>
          <p>
            Therefore, this procedure progressively aligns \( q_{\text{last}} \) and \( r_{\text{last}} \), thereby furthering the self-corruption process.
          </p>
        </div>
        <img src="./static/images/realign.jpg" alt="realign image." style="width: 100%; max-width: 3000px; height: auto; object-fit: contain; margin-top: auto;">
      </div>
    
      <!-- SlipperySlopeParaphrase Section -->
      <div class="column" style="display: flex; flex-direction: column;">
        <div class="content">
          <h2 class="title is-3">SlipperySlopeParaphrase</h2>
          <p>
            When a refusal occurs and the last response \( r_{\text{last}} \) remains aligned with its query \( q_{\text{last}} \), we insert a bridge prompt \( q_{\text{mid}} \) to ease the model into accepting a more harmful request.
          </p>
          <p>
            Specifically, we obtain \( q_{\text{mid}} \gets \text{getMid}(q_{\text{last}}, q_i) \) from an assistant model \( \mathcal{M} \) so that its maliciousness level falls between \( q_{\text{last}} \) and \( q_i \). 
            We then query the target model with \( q_{\text{mid}} \); if the model refuses again, we paraphrase \( q_{\text{mid}} \) repeatedly until acceptance. 
            Once the model provides a valid response \( r_{\text{mid}} \), we incorporate both \( q_{\text{mid}} \) and \( r_{\text{mid}} \) into the chat history \( \mathcal{H} \).
          </p>
          <p>
            This incremental bridging step parallels the <em>foot-in-the-door</em> phenomenon, in which acceptance of a smaller request facilitates compliance with a subsequent, more harmful one.
          </p>
        </div>
        <img src="./static/images/SSP.jpg" alt="FITD image." style="width: 100%; max-width: 3000px; height: auto; object-fit: contain; margin-top: auto;">
      </div>
    </div>
    
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiment Results</h2>
    <div class="content">
      <table class="table is-bordered is-striped is-fullwidth">
        <caption><strong>Table 1:</strong> Attack success rate (ASR) of baseline jailbreak attacks and FITD</caption>
        <thead>
          <tr>
            <th rowspan="2">Method</th>
            <th rowspan="2">Attack</th>
            <th colspan="7">Models</th>
            <th rowspan="2">Avg.</th>
          </tr>
          <tr>
            <th>LLaMA-3.1-8B</th>
            <th>LLaMA-3-8B</th>
            <th>Qwen-2-7B</th>
            <th>Qwen-1.5-7B</th>
            <th>Mistral-v0.2-7B</th>
            <th>GPT-4o-mini</th>
            <th>GPT-4o</th>
          </tr>
        </thead>
        <tbody>
          <!-- Single-Turn Methods -->
          <tr>
            <td rowspan="6"><strong>Single-Turn</strong></td>
            <td>DeepInception</td>
            <td>33%/29%</td> <td>3%/3%</td> <td>22%/29%</td> <td>58%/41%</td> <td>50%/34%</td> <td>19%/13%</td> <td>2%/0%</td> <td>27%/21%</td>
          </tr>
          <tr>
            <td>CodeChameleon</td>
            <td>36%/31%</td> <td>31%/33%</td> <td>25%/30%</td> <td>40%/38%</td> <td>39%/39%</td> <td>36%/26%</td> <td>40%/26%</td> <td>34%/30%</td>
          </tr>
          <tr>
            <td>CodeAttack-Slack</td>
            <td>38%/44%</td> <td>48%/40%</td> <td>48%/50%</td> <td>45%/40%</td> <td>45%/40%</td> <td>30%/20%</td> <td>37%/30%</td> <td>41%/38%</td>
          </tr>
          <tr>
            <td>CodeAttack-List</td>
            <td>67%/58%</td> <td>58%/54%</td> <td>56%/50%</td> <td>40%/39%</td> <td>66%/55%</td> <td>39%/29%</td> <td>27%/28%</td> <td>47%/43%</td>
          </tr>
          <tr>
            <td>CodeAttack-String</td>
            <td>71%/60%</td> <td>45%/59%</td> <td>52%/40%</td> <td>47%/39%</td> <td>79%/59%</td> <td>28%/35%</td> <td>33%/31%</td> <td>51%/46%</td>
          </tr>
          <tr>
            <td>ReNeLLM</td>
            <td>69%/61%</td> <td>62%/50%</td> <td>57%/50%</td> <td>70%/52%</td> <td>74%/63%</td> <td>35%/30%</td> <td>45%/35%</td> <td>59%/49%</td>
          </tr>

          <!-- Multi-Turn Methods -->
          <tr>
            <td rowspan="3"><strong>Multi-Turn</strong></td>
            <td>CoA</td>
            <td>29%/34%</td> <td>22%/28%</td> <td>45%/30%</td> <td>41%/25%</td> <td>43%/36%</td> <td>15%/20%</td> <td>3%/1%</td> <td>28%/25%</td>
          </tr>
          <tr>
            <td>ActorAttack</td>
            <td>63%/53%</td> <td>59%/50%</td> <td>59%/58%</td> <td>52%/54%</td> <td>70%/69%</td> <td>58%/50%</td> <td>52%/53%</td> <td>59%/55%</td>
          </tr>
          <tr class="has-background-success-light">
            <td><strong>FITD</strong></td>
            <td><strong>92%/94%</strong></td> <td><strong>98%/93%</strong></td> <td><strong>95%/93%</strong></td> 
            <td><strong>94%/88%</strong></td> <td><strong>96%/94%</strong></td> <td><strong>95%/93%</strong></td> 
            <td><strong>88%/84%</strong></td> <td><strong>94%/91%</strong></td>
          </tr>
        </tbody>
      </table>
      <p class="has-text-justified">
        <strong>Table 1</strong> shows the attack success rate (ASR) of baseline jailbreak attacks and FITD on JailbreakBench and HarmBench across 7 models.
        Each cell represents ASR values in the format <em>"JailbreakBench / HarmBench"</em>. Higher ASR indicates greater vulnerability to the respective attack.
        The highest ASR for multi-turn attacks is highlighted.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experiment Results</h2>
    <p class="subtitle has-text-centered">Visualization of ASR results under different conditions.</p>

    <div class="columns is-multiline is-centered">
      <!-- 第一行 -->
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/1-transfer.jpg" alt="Transfer Attack">
          <figcaption class="has-text-centered">(a) Transfer attack</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/2-ablation.jpg" alt="Ablation Study">
          <figcaption class="has-text-centered">(b) Ablation study</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/3-defense.jpg" alt="Defense">
          <figcaption class="has-text-centered">(c) Defense</figcaption>
        </figure>
      </div>

      <!-- 第二行 -->
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/4-level.jpg" alt="ASR across different malicious levels">
          <figcaption class="has-text-centered">(d) ASR across different malicious levels</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/5-length.jpg" alt="Harmfulness Score">
          <figcaption class="has-text-centered">(e) Harmfulness of different levels</figcaption>
        </figure>
      </div>
      <div class="column is-one-third">
        <figure class="image">
          <img src="./static/images/6-different.jpg" alt="ASR across different query stages">
          <figcaption class="has-text-centered">(f) ASR across different stages queries</figcaption>
        </figure>
      </div>
    </div>
    <!-- exp1 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (a) Transfer attacks using jailbreak chat histories generated from LLaMA-3.1-8B and GPT-4o-mini as source models on JailbreakBench
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <p>Each module within ISG aligns well with human annotation. For structure, ISG exhibits consistent excellence across all tasks, indicating robust potential for capturing structural requirements in interleaved generation instructions. In both Q-Gen and VQA modules, ISG successfully extracts fine-grained requirements with high fidelity to ground truth.</p>
          
          <p>For the VQA module, the scoring approach consistently outperforms the "Yes-or-No" method, suggesting that more nuanced judgments align better with human evaluations. Vision-guided tasks consistently underperform compared to other tasks, with a noticeable decline in both Q-Gen and VQA modules, underscoring the challenges in automatically evaluating fine-grained aspects of interleaved text-and-image generation.</p>
          
          <p>In holistic evaluation, leveraging a golden answer significantly outperforms the zero-shot judging setting of MLLMs, especially in vision-guided tasks, yielding an average 20% improvement in human agreement.</p>
        </div>
      </div>
    <!-- exp2 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (b) Ablation study of three components in FITD, response alignment (Re-Align), alignment prompt \(p_{align}\), and SlipperySlopeParaphrase(SSP) on JailbreakBench
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <p>As shown in Figure b, removing all three mechanisms (ReAlign, prompt alignment $p_{align}$, and SSP) significantly reduces ASR.
          On LLaMA-3.1, ASR drops from 92% to 75%, while on LLaMA-3, it decreases from 98% to 59%. Similar declines occur in Qwen-2 and Qwen-1.5, dropping to 76% and 80%, respectively. These results indicate that response alignment, prompt alignment, and paraphrasing are critical for maintaining \Tech’s effectiveness. Without them, the attack weakens, especially against models with stronger alignment guardrails.</p> 

          <p>When response and prompt alignment are removed (w/o ReAlign, $p_{align}$), ASR remains high but still declines. On LLaMA-3.1, ASR stays at 91%, suggesting paraphrasing helps retain effectiveness. However, on LLaMA-3, ASR drops from 98% to 63%, showing paraphrasing alone is insufficient against stricter safeguards. Qwen-2 and Qwen-1.5 follow a similar trend, with ASR decreasing to 75% and 81%. While paraphrasing mitigates some loss, it cannot fully replace alignment techniques.</p>   

          <p>Removing only response alignment (w/o ReAlign) has a smaller impact. LLaMA-3.1 and Qwen-2 maintain ASR at 92% and 75%, while LLaMA-3 drops from 98% to 79%. The effect is more pronounced in Qwen-1.5 and Mistral-v0.2, where ASR falls from 94% to 83% and 96% to 90%, respectively. This suggests response alignment gradually erodes safeguards, aligning with the principle of incremental compliance.</p>  

          <p>Overall, response alignment, prompt alignment, and SlipperySlopeParaphrase are essential for high jailbreak success. Response alignment is key to bypassing safeguards, while paraphrasing further weakens model alignment over time.

        </div>
      </div>
    <!-- exp3 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (c) ASR under different defense methods on JailbreakBench
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <p>Figure c shows ASR of Tech across models under different defense strategies.
            OpenAI-Moderation reduces ASR slightly by 3%-8%. LLaMA-Guard-2 offers a stronger defense, lowering ASR to 79%-91%. LLaMA-Guard-3 further improves moderation, achieving the lowest ASR 78%-84%.
            LLaMA-Guard-3 consistently outperforms other methods, but ASR remains significant.
            We speculate that progressively malicious queries and responses bypassed the detector, indicating room for further improvement in moderation techniques.</p>
          
      
        </div>
      </div>
    </div>
    <!-- exp4 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (d) ASR with different malicious levels \( n \) across models
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <p>Each module within ISG aligns well with human annotation. For structure, ISG exhibits consistent excellence across all tasks, indicating robust potential for capturing structural requirements in interleaved generation instructions. In both Q-Gen and VQA modules, ISG successfully extracts fine-grained requirements with high fidelity to ground truth.</p>
          
          <p>For the VQA module, the scoring approach consistently outperforms the "Yes-or-No" method, suggesting that more nuanced judgments align better with human evaluations. Vision-guided tasks consistently underperform compared to other tasks, with a noticeable decline in both Q-Gen and VQA modules, underscoring the challenges in automatically evaluating fine-grained aspects of interleaved text-and-image generation.</p>
          
          <p>In holistic evaluation, leveraging a golden answer significantly outperforms the zero-shot judging setting of MLLMs, especially in vision-guided tasks, yielding an average 20% improvement in human agreement.</p>
        </div>
      </div>
    </div>
    <!-- exp5 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (e) The harmfulness score of responses $r_i$ at $q_i$ in different malicious levels $i$ across models
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <!-- <p>Each module within ISG aligns well with human annotation. For structure, ISG exhibits consistent excellence across all tasks, indicating robust potential for capturing structural requirements in interleaved generation instructions. In both Q-Gen and VQA modules, ISG successfully extracts fine-grained requirements with high fidelity to ground truth.</p> -->
          <p>To assess the impact of increasing the malicious level on the harmfulness of model's responses, we use the chat history of malicious level $n=12$ experiment in Table~\ref{tab:main} and analyze the harmfulness of responses at each level across multiple LLMs.
        The harmfulness is measured by score 1-5, where a higher score indicate greater harmfulness. We report the mean harmfulness scores for each model at malicious level $i$ ranging from 1 to 12.</p>
        <p>Figure d presents the harmfulness scores of responses at different malicious levels for all evaluated models. 
        We use GPT-4o to score each response.</p>
        <p>We observe that the harmfulness scores generally increase with the malicious level. 
        At $i=1$, the harmfulness scores are relatively low, with values around 2.32 on average across models. 
        However, as the level increases, the harmfulness score consistently rises to 4.23 on average at $i=12$.
        These results show that as the malicious level increases, LLMs become more vulnerable and generate more harmful responses, suggesting that model's alignment weakens over time, making it easier for \Tech to bypass safeguards.</p>
         
        </div>
      </div>
    </div>
    <!-- exp6 -->
    <div class="container">
      <div class="experiment-card">
        <div class="experiment-header">
          (f)  ASR versus the number of queries retained for two extraction strategies: Backward Extraction and Forward Extraction
        </div>
        <!-- 将内容放入expandable类中，而不是experiment-content -->
        <div class="expandable">
          <p>Each module within ISG aligns well with human annotation. For structure, ISG exhibits consistent excellence across all tasks, indicating robust potential for capturing structural requirements in interleaved generation instructions. In both Q-Gen and VQA modules, ISG successfully extracts fine-grained requirements with high fidelity to ground truth.</p>
          
          <p>For the VQA module, the scoring approach consistently outperforms the "Yes-or-No" method, suggesting that more nuanced judgments align better with human evaluations. Vision-guided tasks consistently underperform compared to other tasks, with a noticeable decline in both Q-Gen and VQA modules, underscoring the challenges in automatically evaluating fine-grained aspects of interleaved text-and-image generation.</p>
          
          <p>In holistic evaluation, leveraging a golden answer significantly outperforms the zero-shot judging setting of MLLMs, especially in vision-guided tasks, yielding an average 20% improvement in human agreement.</p>
        </div>
      </div>
    </div>


  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
